# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ie3QLcl9XOWvesuXLkeQg98-0snPk2hU
"""

import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load fine-tuned model
@st.cache_resource
def load_model():
    model = AutoModelForCausalLM.from_pretrained("llm-finetuned")
    tokenizer = AutoTokenizer.from_pretrained("llm-finetuned")
    return model, tokenizer

model, tokenizer = load_model()

# Streamlit UI
st.title("ðŸš€ Fine-tuned LLM Chatbot")
st.markdown("Ask anything based on the fine-tuned dataset.")

user_input = st.text_area("Enter your prompt:", height=150)

if st.button("Generate Response"):
    if user_input.strip() == "":
        st.warning("Please enter a prompt.")
    else:
        input_ids = tokenizer(user_input, return_tensors="pt").input_ids
        with torch.no_grad():
            output = model.generate(input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        st.success("Response:")
        st.write(response)